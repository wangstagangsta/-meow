{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Beatgrid CRNN MVP Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These globals configure audio preprocessing and training defaults. `TARGET_SR`, `N_MELS`, `N_FFT`, and `HOP_LENGTH` define how raw audio is turned into log-mel frames that the network ingests, while `BEAT_TOLERANCE_SEC` specifies how close a frame must be to a ground-truth beat to count as positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "TARGET_SR = 44100\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512  # ~11.6 ms at 44.1k\n",
        "BEAT_TOLERANCE_SEC = 0.03  # +/- 30 ms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Label JSON files contain a constant BPM, duration, and optional downbeat offset per track. The helpers below simply load that metadata and turn it into a per-beat timeline so later steps can create frame-wise supervision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Label Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_label_json(label_path: str) -> dict:\n",
        "    with open(label_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def generate_beats_from_constant_bpm(\n",
        "    bpm: float,\n",
        "    duration: float,\n",
        "    downbeat_offset_sec: float = 0.0,\n",
        ") -> List[float]:\n",
        "    \"\"\"Generate a simple beatgrid for a constant-BPM track.\"\"\"\n",
        "    if bpm is None or bpm <= 0:\n",
        "        return []\n",
        "\n",
        "    period = 60.0 / float(bpm)  # seconds per beat\n",
        "    beat_times = []\n",
        "\n",
        "    t = downbeat_offset_sec\n",
        "    while t < duration:\n",
        "        beat_times.append(t)\n",
        "        t += period\n",
        "\n",
        "    return beat_times\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio to Mel Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset takes a list of `{audio_path, label_path}` pairs, converts the audio into log-mel tensors, and aligns constant-BPM label grids to per-frame 0/1 targets. Each `__getitem__` returns `(mel_tensor[T,F], label_tensor[T])`, so batching simply stacks whole tracks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_to_mel(audio_path: str):\n",
        "    \"\"\"Load audio to mono log-mel spectrogram and frame timestamps.\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)\n",
        "    duration = len(y) / sr\n",
        "\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS,\n",
        "        power=2.0,\n",
        "    )\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max).T  # (T, N_MELS)\n",
        "\n",
        "    frames = np.arange(mel_db.shape[0])\n",
        "    frame_times = librosa.frames_to_time(\n",
        "        frames,\n",
        "        sr=sr,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_fft=N_FFT,\n",
        "    )\n",
        "\n",
        "    return mel_db, frame_times, duration\n",
        "\n",
        "\n",
        "def beat_times_to_frame_labels(\n",
        "    beat_times: List[float],\n",
        "    frame_times: np.ndarray,\n",
        "    tolerance_sec: float = BEAT_TOLERANCE_SEC,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Convert beat timestamps to per-frame binary labels.\"\"\"\n",
        "    labels = np.zeros_like(frame_times, dtype=np.float32)\n",
        "\n",
        "    if len(beat_times) == 0:\n",
        "        return labels\n",
        "\n",
        "    beat_idx = 0\n",
        "    num_beats = len(beat_times)\n",
        "\n",
        "    for i, ft in enumerate(frame_times):\n",
        "        while beat_idx + 1 < num_beats and beat_times[beat_idx] < ft:\n",
        "            if abs(beat_times[beat_idx + 1] - ft) < abs(beat_times[beat_idx] - ft):\n",
        "                beat_idx += 1\n",
        "            else:\n",
        "                break\n",
        "        if abs(beat_times[beat_idx] - ft) <= tolerance_sec:\n",
        "            labels[i] = 1.0\n",
        "\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BeatActivationDataset(Dataset):\n",
        "    \"\"\"Dataset returning full-track mel tensors and beat labels.\"\"\"\n",
        "\n",
        "    def __init__(self, items: List[dict]):\n",
        "        self.items = items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]\n",
        "        audio_path = item[\"audio_path\"]\n",
        "        label_path = item[\"label_path\"]\n",
        "\n",
        "        mel_db, frame_times, duration_audio = load_audio_to_mel(audio_path)\n",
        "\n",
        "        label_json = load_label_json(label_path)\n",
        "        bpm = float(label_json[\"bpm\"])\n",
        "        duration_label = float(label_json.get(\"duration\", duration_audio))\n",
        "        downbeat_offset = float(label_json.get(\"downbeatOffset\", 0))\n",
        "\n",
        "        beat_times = generate_beats_from_constant_bpm(\n",
        "            bpm=bpm,\n",
        "            duration=duration_label,\n",
        "            downbeat_offset_sec=downbeat_offset,\n",
        "        )\n",
        "\n",
        "        labels = beat_times_to_frame_labels(beat_times, frame_times)\n",
        "\n",
        "        mel_tensor = torch.from_numpy(mel_db).float()\n",
        "        labels_tensor = torch.from_numpy(labels).float()\n",
        "\n",
        "        return mel_tensor, labels_tensor\n",
        "\n",
        "\n",
        "def collate_full_tracks(batch):\n",
        "    \"\"\"Simple collate that stacks mels/labels (assumes equal lengths).\"\"\"\n",
        "    mels = [b[0] for b in batch]\n",
        "    labels = [b[1] for b in batch]\n",
        "\n",
        "    mel_batch = torch.stack(mels, dim=0)\n",
        "    label_batch = torch.stack(labels, dim=0)\n",
        "\n",
        "    return mel_batch, label_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Discovery & Splits\n",
        "Automatically pair each label JSON with its audio file so the training run always reflects every annotated track, then optionally hold out a few items for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discover_label_items(label_dir: str, audio_dir: str) -> List[dict]:\n",
        "    \"\"\"Pair each label json with its corresponding audio file.\"\"\"\n",
        "    label_dir_path = Path(label_dir)\n",
        "    audio_dir_path = Path(audio_dir)\n",
        "\n",
        "    if not label_dir_path.exists():\n",
        "        raise FileNotFoundError(f\"Label directory not found: {label_dir_path}\")\n",
        "    if not audio_dir_path.exists():\n",
        "        raise FileNotFoundError(f\"Audio directory not found: {audio_dir_path}\")\n",
        "\n",
        "    label_files = sorted(label_dir_path.glob(\"*.labels.json\"))\n",
        "    if not label_files:\n",
        "        raise FileNotFoundError(f\"No *.labels.json files found in {label_dir_path}\")\n",
        "\n",
        "    items: List[dict] = []\n",
        "    missing_audio = []\n",
        "    common_exts = [\"\", \".mp3\", \".m4a\", \".wav\", \".flac\", \".ogg\", \".aif\", \".aiff\"]\n",
        "\n",
        "    for label_path in label_files:\n",
        "        label_data = load_label_json(str(label_path))\n",
        "        file_name = label_data.get(\"fileName\")\n",
        "\n",
        "        candidates = []\n",
        "        if file_name:\n",
        "            candidates.append(audio_dir_path / file_name)\n",
        "\n",
        "        base_name = label_path.stem\n",
        "        if base_name.endswith(\".labels\"):\n",
        "            base_name = base_name[:-len(\".labels\")]\n",
        "\n",
        "        for ext in common_exts:\n",
        "            candidate_path = audio_dir_path / f\"{base_name}{ext}\"\n",
        "            if candidate_path not in candidates:\n",
        "                candidates.append(candidate_path)\n",
        "\n",
        "        audio_path = next((path for path in candidates if path.exists()), None)\n",
        "\n",
        "        if audio_path is None:\n",
        "            missing_audio.append((label_path.name, file_name or f\"{base_name}.*\"))\n",
        "            continue\n",
        "\n",
        "        items.append({\"audio_path\": str(audio_path), \"label_path\": str(label_path)})\n",
        "\n",
        "    if missing_audio:\n",
        "        print(\"Warning: Skipping labels with missing audio files:\")\n",
        "        for label_name, expected in missing_audio:\n",
        "            print(f\" - {label_name} (expected audio similar to {expected})\")\n",
        "\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No valid label/audio pairs were found.\")\n",
        "\n",
        "    return items\n",
        "\n",
        "\n",
        "def split_train_val_items(items: List[dict], val_count: int) -> Tuple[List[dict], List[dict]]:\n",
        "    \"\"\"Split dataset into train/val using the end of the list as validation.\"\"\"\n",
        "    if val_count <= 0:\n",
        "        return items, []\n",
        "\n",
        "    if val_count >= len(items):\n",
        "        print(\n",
        "            f\"Requested val_count={val_count} but only {len(items)} tracks available. \"\n",
        "            \"Reducing validation set so at least one training track remains.\"\n",
        "        )\n",
        "        val_count = max(0, len(items) - 1)\n",
        "\n",
        "    if val_count == 0:\n",
        "        return items, []\n",
        "\n",
        "    train_items = items[:-val_count]\n",
        "    val_items = items[-val_count:]\n",
        "    return train_items, val_items\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`BeatCRNN` first compresses the mel time–frequency map with a small CNN, then runs a bidirectional LSTM so each frame sees past/future context before emitting a beat logit. The rest of the notebook only relies on the logits; applying `torch.sigmoid` converts them to beat activation probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BeatCRNN(nn.Module):\n",
        "    \"\"\"Minimal CRNN mapping log-mels to beat logits.\"\"\"\n",
        "\n",
        "    def __init__(self, n_mels=N_MELS, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
        "        )\n",
        "\n",
        "        freq_out = n_mels // 4\n",
        "        cnn_channels = 64\n",
        "        rnn_input_size = cnn_channels * freq_out\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        self.output = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass producing beat logits.\"\"\"\n",
        "        B, T, F = x.shape\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.cnn(x)\n",
        "        B, C, T_new, F_new = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(B, T_new, C * F_new)\n",
        "        x, _ = self.rnn(x)\n",
        "        logits = self.output(x).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_mvp(\n",
        "    train_items: List[dict],\n",
        "    val_items: List[dict] = None,\n",
        "    num_epochs: int = 20,\n",
        "    lr: float = 1e-3,\n",
        "    device: str = None,\n",
        "):\n",
        "    \"\"\"Train the Beat CRNN on full tracks.\"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_ds = BeatActivationDataset(train_items)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_full_tracks,\n",
        "    )\n",
        "\n",
        "    if val_items is not None and len(val_items) > 0:\n",
        "        val_ds = BeatActivationDataset(val_items)\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_full_tracks,\n",
        "        )\n",
        "    else:\n",
        "        val_loader = None\n",
        "\n",
        "    model = BeatCRNN().to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for mel_batch, label_batch in train_loader:\n",
        "            mel_batch = mel_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            logits = model(mel_batch)\n",
        "\n",
        "            T_pred = logits.shape[1]\n",
        "            T_true = label_batch.shape[1]\n",
        "            T_min = min(T_pred, T_true)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits[:, :T_min],\n",
        "                label_batch[:, :T_min],\n",
        "            )\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / max(1, len(train_loader))\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Train loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for mel_batch, label_batch in val_loader:\n",
        "                    mel_batch = mel_batch.to(device)\n",
        "                    label_batch = label_batch.to(device)\n",
        "\n",
        "                    logits = model(mel_batch)\n",
        "                    T_pred = logits.shape[1]\n",
        "                    T_true = label_batch.shape[1]\n",
        "                    T_min = min(T_pred, T_true)\n",
        "\n",
        "                    loss = criterion(\n",
        "                        logits[:, :T_min],\n",
        "                        label_batch[:, :T_min],\n",
        "                    )\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "            print(f\"           Val loss:   {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "Automatically gather every labeled track, optionally shuffle/hold out validation songs, train the CRNN, and save a checkpoint for inference helpers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Overfitting on 1 track ===\n",
            "Using device: cpu\n",
            "Epoch 1/30 - Train loss: 0.7084\n",
            "Epoch 2/30 - Train loss: 0.4957\n",
            "Epoch 3/30 - Train loss: 0.4409\n",
            "Epoch 4/30 - Train loss: 0.4497\n",
            "Epoch 5/30 - Train loss: 0.4548\n",
            "Epoch 6/30 - Train loss: 0.4481\n",
            "Epoch 7/30 - Train loss: 0.4410\n",
            "Epoch 8/30 - Train loss: 0.4381\n",
            "Epoch 9/30 - Train loss: 0.4394\n",
            "Epoch 10/30 - Train loss: 0.4417\n",
            "Epoch 11/30 - Train loss: 0.4425\n",
            "Epoch 12/30 - Train loss: 0.4414\n",
            "Epoch 13/30 - Train loss: 0.4395\n",
            "Epoch 14/30 - Train loss: 0.4379\n",
            "Epoch 15/30 - Train loss: 0.4369\n",
            "Epoch 16/30 - Train loss: 0.4367\n",
            "Epoch 17/30 - Train loss: 0.4370\n",
            "Epoch 18/30 - Train loss: 0.4372\n",
            "Epoch 19/30 - Train loss: 0.4369\n",
            "Epoch 20/30 - Train loss: 0.4362\n",
            "Epoch 21/30 - Train loss: 0.4352\n",
            "Epoch 22/30 - Train loss: 0.4342\n",
            "Epoch 23/30 - Train loss: 0.4333\n",
            "Epoch 24/30 - Train loss: 0.4324\n",
            "Epoch 25/30 - Train loss: 0.4313\n",
            "Epoch 26/30 - Train loss: 0.4300\n",
            "Epoch 27/30 - Train loss: 0.4284\n",
            "Epoch 28/30 - Train loss: 0.4263\n",
            "Epoch 29/30 - Train loss: 0.4240\n",
            "Epoch 30/30 - Train loss: 0.4215\n",
            "=== Training on all tracks (simple val on last) ===\n",
            "Using device: cpu\n",
            "Epoch 1/30 - Train loss: 0.5088\n",
            "           Val loss:   0.4474\n",
            "Epoch 2/30 - Train loss: 0.4411\n",
            "           Val loss:   0.4421\n",
            "Epoch 3/30 - Train loss: 0.4398\n",
            "           Val loss:   0.4394\n",
            "Epoch 4/30 - Train loss: 0.4383\n",
            "           Val loss:   0.4396\n",
            "Epoch 5/30 - Train loss: 0.4377\n",
            "           Val loss:   0.4382\n",
            "Epoch 6/30 - Train loss: 0.4370\n",
            "           Val loss:   0.4378\n",
            "Epoch 7/30 - Train loss: 0.4357\n",
            "           Val loss:   0.4365\n",
            "Epoch 8/30 - Train loss: 0.4328\n",
            "           Val loss:   0.4347\n",
            "Epoch 9/30 - Train loss: 0.4270\n",
            "           Val loss:   0.4462\n",
            "Epoch 10/30 - Train loss: 0.4158\n",
            "           Val loss:   0.4798\n",
            "Epoch 11/30 - Train loss: 0.3986\n",
            "           Val loss:   0.5177\n",
            "Epoch 12/30 - Train loss: 0.3790\n",
            "           Val loss:   0.4940\n",
            "Epoch 13/30 - Train loss: 0.3513\n",
            "           Val loss:   0.3900\n",
            "Epoch 14/30 - Train loss: 0.3327\n",
            "           Val loss:   0.3630\n",
            "Epoch 15/30 - Train loss: 0.3128\n",
            "           Val loss:   0.3557\n",
            "Epoch 16/30 - Train loss: 0.3049\n",
            "           Val loss:   0.3637\n",
            "Epoch 17/30 - Train loss: 0.2945\n",
            "           Val loss:   0.3437\n",
            "Epoch 18/30 - Train loss: 0.2666\n",
            "           Val loss:   0.3525\n",
            "Epoch 19/30 - Train loss: 0.3010\n",
            "           Val loss:   0.3575\n",
            "Epoch 20/30 - Train loss: 0.3209\n",
            "           Val loss:   0.3435\n",
            "Epoch 21/30 - Train loss: 0.2819\n",
            "           Val loss:   0.4381\n",
            "Epoch 22/30 - Train loss: 0.3511\n",
            "           Val loss:   0.4086\n",
            "Epoch 23/30 - Train loss: 0.3558\n",
            "           Val loss:   0.3980\n",
            "Epoch 24/30 - Train loss: 0.3050\n",
            "           Val loss:   0.3681\n",
            "Epoch 25/30 - Train loss: 0.2727\n",
            "           Val loss:   0.3328\n",
            "Epoch 26/30 - Train loss: 0.2609\n",
            "           Val loss:   0.3412\n",
            "Epoch 27/30 - Train loss: 0.2604\n",
            "           Val loss:   0.3328\n",
            "Epoch 28/30 - Train loss: 0.2427\n",
            "           Val loss:   0.3278\n",
            "Epoch 29/30 - Train loss: 0.2569\n",
            "           Val loss:   0.3212\n",
            "Epoch 30/30 - Train loss: 0.2616\n",
            "           Val loss:   0.3323\n"
          ]
        }
      ],
      "source": [
        "# Resolve project/data directories whether the notebook is launched from\n",
        "# repo root or the notebooks/ subdirectory.\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "data_root = PROJECT_ROOT / \"data\"\n",
        "audio_dir = data_root / \"audio\"\n",
        "label_dir = data_root / \"labels\"\n",
        "\n",
        "# Training hyperparameters / knobs\n",
        "VAL_COUNT = 2          # hold out this many tracks for validation (taken from end)\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "SHUFFLE_SEED = None    # e.g. 42 to randomize order deterministically\n",
        "DEVICE = auto          # None => auto (cuda if available)\n",
        "CHECKPOINT_PATH = PROJECT_ROOT / \"beat_crnn_mvp.pth\"\n",
        "\n",
        "all_items = discover_label_items(str(label_dir), str(audio_dir))\n",
        "\n",
        "if SHUFFLE_SEED is not None:\n",
        "    random.seed(SHUFFLE_SEED)\n",
        "    random.shuffle(all_items)\n",
        "\n",
        "train_items, val_items = split_train_val_items(all_items, VAL_COUNT)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Found {len(all_items)} labeled tracks.\")\n",
        "print(f\"Training set size: {len(train_items)}\")\n",
        "print(\n",
        "    f\"Validation set size: {len(val_items)}\"\n",
        "    if val_items\n",
        "    else \"Validation set size: 0 (validation disabled)\"\n",
        ")\n",
        "\n",
        "model = train_mvp(\n",
        "    train_items=train_items,\n",
        "    val_items=val_items,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "CHECKPOINT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "print(f\"Saved checkpoint to {CHECKPOINT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Beat Grid\n",
        "This section converts beat activations into musical timing information. Given a track’s activation curve we (1) estimate the BPM, (2) refine the downbeat offset, and (3) output a constant-BPM beatgrid annotated with per-beat confidence samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_best_offset_from_activation(\n",
        "    frame_times,\n",
        "    probs,\n",
        "    bpm,\n",
        "    duration,\n",
        "    search_window=None,\n",
        "    n_offsets=200,\n",
        "):\n",
        "    \"\"\"\n",
        "    Search over candidate offsets in [0, period) and pick the one whose\n",
        "    ideal grid best aligns with high activations.\n",
        "\n",
        "    frame_times: (T,) seconds\n",
        "    probs: (T,) beat probabilities\n",
        "    bpm: float\n",
        "    duration: float (seconds)\n",
        "    search_window: (t_start, t_end) or None\n",
        "        Only use this region for scoring (e.g. skip very quiet intro).\n",
        "    n_offsets: number of offset candidates to test.\n",
        "\n",
        "    Returns:\n",
        "        best_offset: float in [0, period)\n",
        "        best_score: float mean activation score for that offset.\n",
        "    \"\"\"\n",
        "    period = 60.0 / float(bpm)\n",
        "\n",
        "    # Restrict to region for scoring (optional)\n",
        "    ft = np.asarray(frame_times)\n",
        "    pb = np.asarray(probs)\n",
        "\n",
        "    if search_window is not None:\n",
        "        t_start, t_end = search_window\n",
        "        mask = (ft >= t_start) & (ft <= t_end)\n",
        "        ft_score = ft[mask]\n",
        "        pb_score = pb[mask]\n",
        "        dur_score = min(t_end, duration) - t_start\n",
        "    else:\n",
        "        ft_score = ft\n",
        "        pb_score = pb\n",
        "        dur_score = duration\n",
        "\n",
        "    if dur_score <= 0 or len(ft_score) == 0:\n",
        "        # fallback\n",
        "        return 0.0, -np.inf\n",
        "\n",
        "    # Candidate offsets from [0, period)\n",
        "    offsets = np.linspace(0.0, period, num=n_offsets, endpoint=False)\n",
        "    best_score = -np.inf\n",
        "    best_offset = 0.0\n",
        "\n",
        "    for off in offsets:\n",
        "        # grid times within score window\n",
        "        if search_window is not None:\n",
        "            t0 = max(off, search_window[0])\n",
        "        else:\n",
        "            t0 = off\n",
        "        if t0 > duration:\n",
        "            continue\n",
        "\n",
        "        grid_times = np.arange(t0, duration, period)\n",
        "\n",
        "        # restrict to scoring window\n",
        "        if search_window is not None:\n",
        "            grid_times = grid_times[\n",
        "                (grid_times >= search_window[0]) & (grid_times <= search_window[1])\n",
        "            ]\n",
        "\n",
        "        if len(grid_times) == 0:\n",
        "            continue\n",
        "\n",
        "        # sample activation at grid times\n",
        "        grid_probs = sample_probs_at_times(ft_score, pb_score, grid_times)\n",
        "\n",
        "        # score: mean prob (you can also use sum or top-k mean)\n",
        "        score = grid_probs.mean()\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_offset = off\n",
        "\n",
        "    return best_offset, best_score\n",
        "\n",
        "\n",
        "def sample_probs_at_times(frame_times, probs, query_times):\n",
        "    \"\"\"\n",
        "    Given frame_times (T,), probs (T,) and query_times (K,),\n",
        "    return probs at nearest frame for each query time.\n",
        "    \"\"\"\n",
        "    frame_times = np.asarray(frame_times)\n",
        "    probs = np.asarray(probs)\n",
        "    query_times = np.asarray(query_times)\n",
        "\n",
        "    indices = np.searchsorted(frame_times, query_times, side=\"left\")\n",
        "    indices = np.clip(indices, 0, len(frame_times) - 1)\n",
        "\n",
        "    # Optional: for middle-between-frames, pick closer of left/right\n",
        "    left_indices = np.clip(indices - 1, 0, len(frame_times) - 1)\n",
        "    right_indices = indices\n",
        "    left_d = np.abs(frame_times[left_indices] - query_times)\n",
        "    right_d = np.abs(frame_times[right_indices] - query_times)\n",
        "    use_left = left_d < right_d\n",
        "    final_indices = np.where(use_left, left_indices, right_indices)\n",
        "\n",
        "    return probs[final_indices]\n",
        "\n",
        "\n",
        "def estimate_bpm_from_activation(\n",
        "    frame_times,\n",
        "    probs,\n",
        "    duration,\n",
        "    bpm_min=70,\n",
        "    bpm_max=200,\n",
        "    bpm_step=0.5,\n",
        "    search_window=None,\n",
        "    n_offsets=120,\n",
        "):\n",
        "    \"\"\"\n",
        "    Grid-search BPM by evaluating how well a constant beatgrid aligns with the\n",
        "    activation curve. Returns (best_bpm, score).\n",
        "    \"\"\"\n",
        "    candidate_bpms = np.arange(bpm_min, bpm_max + 1e-6, bpm_step)\n",
        "    best_bpm = None\n",
        "    best_score = -np.inf\n",
        "\n",
        "    for bpm in candidate_bpms:\n",
        "        if bpm <= 0:\n",
        "            continue\n",
        "        _, score = find_best_offset_from_activation(\n",
        "            frame_times,\n",
        "            probs,\n",
        "            bpm,\n",
        "            duration,\n",
        "            search_window=search_window,\n",
        "            n_offsets=n_offsets,\n",
        "        )\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_bpm = float(bpm)\n",
        "\n",
        "    if best_bpm is None:\n",
        "        best_bpm = float(bpm_min)\n",
        "\n",
        "    return best_bpm, best_score\n",
        "\n",
        "\n",
        "def build_beatgrid_from_activation(\n",
        "    frame_times,\n",
        "    probs,\n",
        "    bpm,\n",
        "    duration,\n",
        "    search_window=None,\n",
        "    best_offset=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Given beat activation, BPM and duration, produce a beatgrid.\n",
        "\n",
        "    Returns:\n",
        "        beat_grid: [t0, t1, ...] grid times (seconds)\n",
        "        info_per_beat: list of dicts with optional confidence etc.\n",
        "    \"\"\"\n",
        "    period = 60.0 / float(bpm)\n",
        "\n",
        "    if best_offset is None:\n",
        "        best_offset, _ = find_best_offset_from_activation(\n",
        "            frame_times, probs, bpm, duration, search_window=search_window\n",
        "        )\n",
        "\n",
        "    # Generate full grid\n",
        "    t = best_offset\n",
        "    beat_grid = []\n",
        "    while t < duration + 1e-6:\n",
        "        beat_grid.append(float(t))\n",
        "        t += period\n",
        "\n",
        "    beat_grid = np.array(beat_grid)\n",
        "\n",
        "    # For each grid beat, find nearest activation frame + prob\n",
        "    probs_at_grid = sample_probs_at_times(frame_times, probs, beat_grid)\n",
        "\n",
        "    info_per_beat = []\n",
        "    for t_grid, p in zip(beat_grid, probs_at_grid):\n",
        "        info_per_beat.append(\n",
        "            {\n",
        "                \"time\": float(t_grid),\n",
        "                \"prob_at_grid\": float(p),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return beat_grid, info_per_beat, best_offset\n",
        "\n",
        "\n",
        "def compute_activation_for_track(model, audio_path):\n",
        "    \"\"\"\n",
        "    Run model on a track and return (frame_times, probs, duration).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    mel_db, frame_times, duration = load_audio_to_mel(audio_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(mel_db).float().unsqueeze(0).to(device)\n",
        "        logits = model(x)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    # match frame_times length to probs (due to CNN pooling)\n",
        "    frame_times = frame_times[: len(probs)]\n",
        "\n",
        "    return frame_times, probs, duration\n",
        "\n",
        "\n",
        "def infer_bpm_offset_and_grid(\n",
        "    model,\n",
        "    audio_path,\n",
        "    bpm_min=70,\n",
        "    bpm_max=200,\n",
        "    bpm_step=0.5,\n",
        "    search_window=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Convenience wrapper that runs the model, estimates BPM + downbeat offset,\n",
        "    and returns the full beatgrid plus raw activation.\n",
        "    \"\"\"\n",
        "    frame_times, probs, duration = compute_activation_for_track(model, audio_path)\n",
        "\n",
        "    best_bpm, bpm_score = estimate_bpm_from_activation(\n",
        "        frame_times,\n",
        "        probs,\n",
        "        duration,\n",
        "        bpm_min=bpm_min,\n",
        "        bpm_max=bpm_max,\n",
        "        bpm_step=bpm_step,\n",
        "        search_window=search_window,\n",
        "    )\n",
        "\n",
        "    best_offset, _ = find_best_offset_from_activation(\n",
        "        frame_times,\n",
        "        probs,\n",
        "        best_bpm,\n",
        "        duration,\n",
        "        search_window=search_window,\n",
        "    )\n",
        "\n",
        "    beat_grid, info_per_beat, _ = build_beatgrid_from_activation(\n",
        "        frame_times,\n",
        "        probs,\n",
        "        best_bpm,\n",
        "        duration,\n",
        "        search_window=search_window,\n",
        "        best_offset=best_offset,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bpm\": best_bpm,\n",
        "        \"bpm_score\": bpm_score,\n",
        "        \"offset\": best_offset,\n",
        "        \"beat_grid\": beat_grid,\n",
        "        \"info_per_beat\": info_per_beat,\n",
        "        \"frame_times\": frame_times,\n",
        "        \"probs\": probs,\n",
        "        \"duration\": duration,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running The Beatgrid Pipeline\n",
        "`infer_bpm_offset_and_grid` glues everything together: it runs the CRNN, estimates BPM from the activation curve, finds the downbeat offset, and samples the final beatgrid. The returned dict also exposes the raw activation so you can build your own visualizations or compare against label JSONs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mplx3\\AppData\\Local\\Temp\\ipykernel_28092\\3522242433.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=TARGET_SR, mono=True)\n",
            "c:\\Users\\mplx3\\Documents\\GitHub\\meow\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated BPM: 150.00\n",
            "Estimated downbeat offset: 0.056s\n",
            "Num beats: 436\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.056, 0.456, 0.856, 1.256, 1.656, 2.056, 2.456, 2.856, 3.256,\n",
              "       3.656])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"beat_crnn_mvp.pth\", map_location=\"cpu\"))\n",
        "audio_path = \"data/audio/Open Your Mind.m4a\"\n",
        "\n",
        "results = infer_bpm_offset_and_grid(\n",
        "    model,\n",
        "    audio_path,\n",
        "    bpm_min=120,\n",
        "    bpm_max=190,\n",
        "    bpm_step=0.5,\n",
        ")\n",
        "\n",
        "print(f\"Estimated BPM: {results['bpm']:.2f}\")\n",
        "print(f\"Estimated downbeat offset: {results['offset']:.3f}s\")\n",
        "print(f\"Num beats: {len(results['beat_grid'])}\")\n",
        "results[\"beat_grid\"][:10]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
